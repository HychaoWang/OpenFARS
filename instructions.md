下面给你一份“可复刻 FARS 类系统”的工程化 planning（偏研究系统、可逐步落地）。我会按**目标→架构→模块→里程碑→风险/评估**来写，确保你能直接开工。

------

## 0) 先定边界（否则必失败）

**建议你复刻的是“端到端科研流水线”而不是“100篇论文产出”。**
第一阶段把目标收敛到：

- **领域**：只做一个子领域（比如 *RLVR / LLM eval / data synthesis / prompt optimization* 选一）
- **输出形式**：不是 arXiv 论文，先做 **“短报告 + 可复现实验脚本 + 日志”**
- **研究类型**：只做“可程序化验证”的研究（能自动评分/自动对比）

------

## 1) 系统总体架构（最小可行）

你需要 4 个 agent + 1 个共享工作区 + 1 个执行/资源层：

### A. 共享工作区（blackboard）

一个目录就是一个 project。所有 agent 只通过文件交互（像 FARS 一样）。
推荐目录规范：

- `00_brief/` 研究方向与约束、目标指标
- `01_lit/` 检索到的文献摘要、引用库（bib）
- `02_hypotheses/` 候选假设（结构化 json/yaml）
- `03_plan/` 实验计划（变量、对照、预算）
- `04_runs/` 每次运行的配置、日志、结果
- `05_writeup/` 报告草稿、图表、结论
- `meta/` 审计、decision trace、成本统计

### B. 四类 agent（职责强隔离）

- **Ideation Agent**：文献检索→生成假设→做“可行性/新颖性/可验证性”初筛
- **Planning Agent**：把假设转成实验矩阵（变量、对照、数据、指标、统计检验）
- **Experiment Agent**：执行、调度、失败重试、产出图表与表格
- **Writing Agent**：把结果写成短报告（包含负结果与局限）

### C. 执行层（必须做成“工具接口”）

- `paper_search()`：arXiv/semantic scholar 等（先用简单爬+缓存即可）
- `repo_search()`：GitHub/公开代码（可选）
- `run_job()`：本地/集群/云（先从本地 + 单机 GPU 起步）
- `eval()`：自动评测与统计
- `plot()`：生成可复用图

------

## 2) 关键设计：让系统“能自我推进”的机制

### 2.1 结构化产物（别让 agent 输出散文）

每个环节都要求结构化文件作为交付件，比如：

**Hypothesis schema（例）**

- 背景/动机
- 具体假设（可证伪）
- 预期提升点
- 可验证指标
- 最小实验（MVP）
- 失败条件（何时判负）
- 计算预算上限

**Experiment plan schema**

- 变量/对照
- 数据集与预处理
- 模型与超参范围
- 指标、统计检验
- 消融列表
- 资源预算（GPU小时、batch数、最大重试）

### 2.2 自动审稿/质检（不然会大量垃圾项目）

做一个轻量 “Reviewer/Guardrail” 模块（可以是一个 agent 或规则 + LLM）：

- 可验证性（是否能自动评分）
- 计算预算是否超标
- 是否已有明显已有结论（lit 冲突检测）
- 是否定义了对照组与失败判据

通过才允许进入 `03_plan/`。

### 2.3 失败处理（研究系统的生命线）

Experiment Agent 必须支持：

- 失败分类：OOM / 数据缺失 / 指标异常 / 训练不收敛
- 自动调整：减 batch、缩模型、改学习率、减少 steps
- 最大重试次数 + 记录 decision trace

------

## 3) 分阶段里程碑（推荐 3 个版本）

### V0：单项目端到端（2–4 周级别工作量）

**目标**：对一个固定 research question，跑通全链路一次。

交付：

- 1 个 project 目录完整闭环
- 3–5 个假设 → 1 个被执行 → 1 份短报告（含图表）
- 成本统计（token、GPU、wall time）

实现重点：

- 共享目录协议
- Planning→Experiment 的自动执行
- 日志与可复现（seed/config/环境）

### V1：多项目队列 + 自动筛选（再 3–6 周）

**目标**：每天能稳定跑 3–10 个小实验。

交付：

- 任务队列（FIFO + 优先级）
- 自动审稿 gate
- 失败重试与降级策略
- 统一评测与结果汇总 dashboard（哪怕是 markdown 汇总也行）

### V2：规模化 + 更强评估（再 6–10 周）

**目标**：质量分布可观察、可对比、可回溯。

交付：

- 跨项目对比（同类假设聚类、重复检测）
- “知识库化”：把每个结论存成可检索条目（hypothesis→evidence）
- 人类抽检界面（只抽 top-K 有价值的）

------

## 4) 你需要的技术栈建议（务实版）

- **语言/框架**：Python
- **工作区与状态机**：Pydantic + YAML/JSON + 简单 state machine（别一上来搞复杂编排）
- **任务调度**：本地先用 `asyncio + subprocess`；再上 Ray/Celery/Slurm
- **实验追踪**：W&B 或 MLflow（最少也要存 config+metrics）
- **检索**：先用 arXiv API + 本地向量库（FAISS）缓存摘要
- **统计/绘图**：scipy + matplotlib

------

## 5) 评估指标（否则你不知道系统有没有进步）

至少要有三类：

1. **产出效率**

- 每个 project 从 ideation→writeup 的时间
- 成本：token、GPU小时、失败率

1. **科学有效性（自动可测）**

- 是否有明确对照
- 统计显著性/置信区间
- 复现成功率（重新跑是否一致）

1. **质量信号（半自动）**

- novelty proxy（与检索到文献的距离）
- 写作一致性、结论与图表是否匹配
- 人类抽检打分（每周抽 10 篇）

------

## 6) 安全与责任（你真做公开部署时必须有）

- 输出必须标注 AI 生成
- 设定禁止方向（生物、化学、武器、黑客等高风险）
- 公开前做人审（哪怕是你自己 + 同学两人）
- 记录全流程 trace，方便追责与复盘

------

## 7) 你现在就能开始的“第一步清单”（一天内能做完）

1. 选一个子领域 + 一个数据集 + 一个主指标
2. 写 `00_brief/brief.md`（约束、预算、失败判据）
3. 定义 `hypothesis.json` 与 `plan.json` 的 schema
4. 搭一个最简 pipeline：`ideate → plan → run → write`（哪怕每步先手工触发）

------

如果你愿意更快落地：把你想复刻的**具体研究方向**告诉我（比如“RLVR 在数学推理上的 reward 设计”或“LLM-as-a-judge 的偏差校准”），我可以直接给你一份 **V0 的 project 模板**（目录结构 + schema 示例 + 每个 agent 的 prompt/接口定义 + 执行流程图）。**你更想做哪一类方向？**